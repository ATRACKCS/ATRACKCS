{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d8a545c-e479-4676-b26d-71835583ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import xarray as xr  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import convolve\n",
    "import geopandas as gpd\n",
    "from datetime import timedelta\n",
    "import rioxarray\n",
    "import rasterio\n",
    "from geopy.distance import geodesic\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numbers as checknumbers\n",
    "from shapely.geometry import MultiPolygon, Polygon, shape, Point, MultiPoint, mapping \n",
    "from shapely.wkt import loads\n",
    "from shapely.ops import unary_union\n",
    "import uuid\n",
    "import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import copy\n",
    "import cftime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "034f6fc6-32a0-4642-a5b7-80be7d938109",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTM_WORLD_ZONE = 3857 \n",
    "UTC_LOCAL_HOUR = 0\n",
    "\n",
    "\n",
    "##----------------------------------------------------------Executing ATRACKCS for intercomparisson data\n",
    "# pathResultados = sys.argv[1]\n",
    "# path_obs = sys.argv[2] \n",
    "# run = sys.argv[3]\n",
    "\n",
    "pathResultados = '/home/vrobledodelgado/hpchome/MCSMIP/AGAIN/OUTPUTS/WINTER/SCREAM/'\n",
    "path_obs = '/home/vrobledodelgado/LSS/Humberto/Vanessa/'\n",
    "run = 'model'\n",
    "filename = 'olr_pcp_Winter_SCREAM.nc'\n",
    "\"\"\"\n",
    "Version 3:\n",
    "\n",
    "Dates for winter: 2020-01-20 to 2020-02-28\n",
    "Dates for summer: 2016-08-01 to 2016-09-10\n",
    "\n",
    "Common MCS criteria: \n",
    "The criteria below are guidelines for defining an MCS and each tracker should \n",
    "try to implement them as closely as possible.\n",
    "The area with Tb <= 241 K must be at least 40,000 km2  for at least 4 continuous\n",
    " hours\n",
    "\n",
    "Minimum peak precipitation of 10 mm h-1 at least one pixel (0.1° x 0.1° for 1 hour)\n",
    " inside the whole MCS for 4 continuous hours\n",
    "    Luego selecciono aquellos que tengan al meno 1 pixel con 10 mm/h en un pixel\n",
    "    por al menos 4 horas\n",
    "\n",
    "20,000 km2 mm h-1  (e.g., 100 km x 100 km x 2 mm h-1 ) minimum rainfall volume at least once in the lifetime of the MCS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "LOCATION_FOLIUM = [10, 135]\n",
    "tb_value = 241\n",
    "area_value = 5000\n",
    "pp_rates = 4\n",
    "variables = \"Both\"\n",
    "\n",
    "tb_overshoot = 225\n",
    "duration = 4\n",
    "buffer_threshold = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "517c90a5-c88c-4d52-8dcc-528534ea95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ______________ reading data________________________________________________\n",
    "def read_MCSMIP_data(path_data, run):\n",
    "    \"\"\"\n",
    "    Function for reading Tb and P DataArrays in files. \n",
    "    The spatial resampling is 0.1° - lineal interpolation.\n",
    "    The temporal resampling is 1 h - nearest original coordinate to up-sampled frequency coordinates.\n",
    "    \n",
    "    Inputs:\n",
    "    pathTb: Path where the Tb raw data are located.\n",
    "    pathP: Path where the P raw data are located.\n",
    "    The path must have the next structure:\n",
    "    linux: r\"/home....../\"\n",
    "    windows: r\"C:/....../\"\n",
    "    \n",
    "    Outputs:\n",
    "    Geodataframe with the polygons associated to the P and Tb spots.\n",
    "    \"\"\"\n",
    "    #Reading P data\n",
    "    ds_combined = xr.open_mfdataset(path_obs+filename)\n",
    "    ds = ds_combined.rename(name_dict={'precipitation':'P'})\n",
    "\n",
    "    if run == 'model':\n",
    "        a = 1.228\n",
    "        b = -1.106e-3\n",
    "        sigma = 5.67e-8 # W m^-2 K^-4\n",
    "        tf = (ds.olr/sigma)**0.25\n",
    "        Tb = (-a + np.sqrt(a**2 + 4*b*tf))/(2*b)\n",
    "        ds['olr'] = Tb\n",
    "        ds = ds.rename(name_dict={'olr':'Tb'})\n",
    "        try:\n",
    "            ds = ds.drop_vars('xtime_bnds')\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\", e)\n",
    "            pass\n",
    "        try:\n",
    "            ds = ds.drop_vars('time_bnds')\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\", e)\n",
    "            pass\n",
    "\n",
    "        #Converting the UTC to local hour\n",
    "    datex = ds.time.coords.to_index()\n",
    "    #Replacing the datetimeindex based on UTC_LOCAL_HOUR\n",
    "    datedt = datex.to_pydatetime()\n",
    "    dates_64 = [np.datetime64(row) for row in datedt]\n",
    "    ds =  ds.assign_coords({\"time\": dates_64})\n",
    "    \n",
    "    #Extracting dimensiones: time, lat and lon\n",
    "    dates = ds.time.values; \n",
    "    lon, lat = np.float32(np.meshgrid(ds.lon,ds.lat))\n",
    " \n",
    "    #Establishing EPSG:4326 ACA hay que revisar que projection plana funciona para todo el globo\n",
    "    ds = ds.rio.set_crs(4326)\n",
    "    ds.attrs['crs'] = ds.rio.crs\n",
    "    \n",
    "    initial_date_lecture = str(ds.time[0].values)[:16]\n",
    "    final_date_lecture = str(ds.time[-1].values)[:16]\n",
    "    \n",
    "    print('Complete data reading ' + initial_date_lecture + \" - \" + final_date_lecture)    \n",
    "    return ds, dates, lon, lat\n",
    "\n",
    "def polygon_identify(data_id, variable):\n",
    "    \"\"\"\n",
    "    Function to draw polygons associated to each P or Tb spot; the polygon delimitation \n",
    "    is from the Convex Hull. This function execute inside the function identify_msc2. \n",
    "    \n",
    "    Inputs:\n",
    "    data_id = DataArray associated with each variable: P or Tb\n",
    "    variable = string name variable: \"Tb\" or \"P\"\n",
    "    \n",
    "    Outputs:\n",
    "    Geodataframe with the polygons associated to the P and tb spots.\n",
    "    \"\"\"\n",
    "    #structure that determines which elements are neighbors to each pixel.\n",
    "    structure = ndimage.generate_binary_structure(2,2)\n",
    "\n",
    "    for step in data_id.time:\n",
    "        # print(data_id)\n",
    "        data_id_temp = data_id.sel(time = step)\n",
    "        arr_filt = ndimage.binary_closing(data_id_temp.values, structure=np.ones((3,3))).astype(data_id_temp.values.dtype) ## llena huecos\n",
    "        arr_filt = ndimage.binary_opening(arr_filt, structure=np.ones((3,3))).astype(arr_filt.dtype) ## borra outliers\n",
    "        blobs, number_of_blobs = ndimage.label(arr_filt, structure=structure)\n",
    "        distance = ndimage.distance_transform_edt(blobs)\n",
    "    \n",
    "        #only the outline of the polygons (spots) based on binary structure.\n",
    "        distance[distance != 1] = 0\n",
    "        blobs[distance == 0]= 0\n",
    "        blobs = blobs.astype('float')\n",
    "        #blobs[blobs  == 0] = np.nan\n",
    "        \n",
    "        #replacing the values generated in the original Datarray\n",
    "        data_id.loc[dict(time = step)] = blobs\n",
    "      \n",
    "    data_id = data_id.where(data_id != 0, np.nan)         \n",
    "    #___________________Polygon generation of the spots______________________________\n",
    "        \n",
    "    #Datarray to Dataframe\n",
    "    df = data_id.to_dataframe().reset_index()\n",
    "    df.dropna(inplace = True)\n",
    "    #Dataframe to Geodataframe\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "            df[[\"time\", variable]], geometry=gpd.points_from_xy(df.lon,df.lat))\n",
    "     \n",
    "    #Extracting the coordinates of the georeferenced points in a separate column\n",
    "    gdf['geo'] = gdf['geometry'].apply(lambda x: x.coords[0])\n",
    "    \n",
    "    #Converting the points-groups to polygons based on the Convex Hull\n",
    "    df = gdf.groupby(by=[\"time\" , variable])['geo'].apply(lambda x: MultiPoint(sorted(x.tolist()))).to_frame().reset_index()\n",
    "    df[\"geo\"] = df[\"geo\"].apply(lambda x: x.convex_hull.wkt).to_frame()\n",
    "    df[\"geo\"] = df[\"geo\"].apply(lambda x: loads(x)).to_frame()\n",
    "    \n",
    "    #Making the Geodataframe final \n",
    "    gdf = gpd.GeoDataFrame(df, geometry=df.geo, crs=4326); del df; del gdf[\"geo\"]\n",
    "    #Dropping lines and solitary points from the Geodataframe\n",
    "    gdf = gdf.loc[gdf['geometry'].geom_type=='Polygon']\n",
    "         \n",
    "    return gdf \n",
    "    \n",
    "def merge_neighbours(gdf_tb,ds,buffer_threshold):\n",
    "    ##create a empy gdf \n",
    "    gdf_merged = gpd.GeoDataFrame(columns=[\"time\", \"geometry\"], crs=\"EPSG:4326\")\n",
    "\n",
    "    for step in ds.time.values:\n",
    "        test = gdf_tb[gdf_tb['time']== step]\n",
    "        \n",
    "        proximity_threshold = buffer_threshold\n",
    "        test['buffered'] = test.geometry.buffer(proximity_threshold)\n",
    "        \n",
    "        merged_geometry = unary_union(test['buffered'])\n",
    "        merged_polygons = [polygon for polygon in merged_geometry.geoms] if merged_geometry.geom_type == 'MultiPolygon' else [merged_geometry]\n",
    "        \n",
    "        merged_gdf = gpd.GeoDataFrame(geometry=merged_polygons, crs=test.crs)\n",
    "        merged_gdf['time'] = step\n",
    "        gdf_merged = pd.concat([gdf_merged, merged_gdf], ignore_index=True)\n",
    "    \n",
    "    gdf_merged = gdf_merged.reset_index(drop=True)\n",
    "    gdf_merged['Tb'] = np.arange(1,len(gdf_merged)+1)\n",
    "    \n",
    "    return gdf_merged\n",
    "\n",
    "def identify_msc2(data, variables,Tb, area_Tb, pp_rates, buffer_threshold):\n",
    "    \"\"\"\n",
    "    Function for identifying the spots, in a time step, based on the methodology\n",
    "    of brightness temperature and precipitation association.\n",
    " \n",
    "    Inputs\n",
    "    data = DataArray of the variables\n",
    "    variables: Methodology of association \"Tb\" or \"Both\":Tb and P. \n",
    "    If \"Tb\" the spots area are delimited by brightness temperature cold cloud top\n",
    "    Elif \"Both\" the spots area are delimited by brightness temperature cold cloud top and\n",
    "    precipitation threshold in each spot\n",
    "    pp_rates = precipitation threshold\n",
    "\n",
    "\n",
    "    #default parameters based on literature\n",
    "    Tb (Brightness Temperature): spots based on limited threshold cold cloud top (ex. <225 K)[Feng et al.,(2021); Li et al.,(2020)]\n",
    "    area_Tb: spots with a minimun largest area polygon (ex. > 2000 km2)[Lui et al., (2019); Vizy & Cook,(2018)] \n",
    "    P (Precipitation): By default is the spots greater than 2 mm/h with length[Feng et al.,(2021)].         \n",
    "\n",
    "    Outputs:\n",
    "    Geodataframe with the polygons associated to the methodology choosed\n",
    "    \"\"\" \n",
    "    kernel = np.ones((5, 5))    \n",
    "    global counter, storms_counter\n",
    "    if variables == \"Both\":\n",
    "        # Masking the DataArray based on limited threshold cold cloud top\n",
    "        mask_tb = (data[\"Tb\"] <= Tb).astype(int)\n",
    "        \n",
    "        # Convert mask to DataArray and establish CRS\n",
    "        datax = mask_tb.to_dataset(name=\"Tb\")\n",
    "        datax = datax.rio.write_crs(4326)\n",
    "    \n",
    "    #selecting Tb variable\n",
    "    datax = datax['Tb']\n",
    "    datax[:,0, :] = 0          # First row\n",
    "    datax[:,-1, :] = 0         # Last row\n",
    "    datax[:,:, 0] = 0          # First column\n",
    "    datax[:,:, -1] = 0 \n",
    "    \n",
    "    data_p = data['P']\n",
    "    #Identifying polygons of Tb and P of the original dataArray  \n",
    "    gdf_tb = polygon_identify(datax, variable = \"Tb\")\n",
    "\n",
    "    gdf_tb2 = merge_neighbours(gdf_tb,datax,buffer_threshold)\n",
    "\n",
    "    #Establishing world mercator\n",
    "    gdf = gdf_tb2.to_crs(UTM_WORLD_ZONE) \n",
    "    \n",
    "    #Calculating polygon area   \n",
    "    gdf[\"area_tb\"] = gdf['geometry'].area #meters\n",
    "    gdf[\"area_tb\"] = gdf[\"area_tb\"]/10**6 #meters to kilometers\n",
    "    gdf[\"area_tb\"] = gdf[\"area_tb\"].round(1)\n",
    "    \n",
    "    #Dropping polygons with an area less than area_Tb\n",
    "    gdf = gdf.loc[gdf['area_tb'] >= area_Tb]    \n",
    "    \n",
    "    #Calculating centroids\n",
    "    gdf[\"centroides\"] = gdf.geometry.centroid\n",
    "    gdf.reset_index(inplace = True, drop = True)\n",
    "        \n",
    "    print(\"Spots identification completed\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def clip_tb(sup, ds,tb_overshoot):\n",
    "    \"\"\"\n",
    "    Function for estimating max, mean and min brightness temperature of each spot \n",
    "    \n",
    "    Inputs\n",
    "    sup: GeodataFrame containing the Tb polygons generated in the process \"identify_msc2\".\n",
    "    ds = DataArray associated with the variables P and Tb resulting from the process 'read_resample_data'.\n",
    "    \n",
    "    Outputs:\n",
    "    GeoDataFrame of the identified polygons with the Tb features\n",
    "    \"\"\"\n",
    "    #Preparing the DataArray\n",
    "    data_magnitud = ds.copy()\n",
    "    data_magnitud = data_magnitud.to_dataframe()\n",
    "    data_magnitud_ = data_magnitud.to_xarray()[\"Tb\"]\n",
    "    \n",
    "    #Establishing EPSG:4326 - WGS geodetic coordinate system \n",
    "    data_magnitud_ = data_magnitud_.rio.write_crs(4326)  \n",
    "    \n",
    "    #Renaming dimensions \n",
    "    data_magnitud_= data_magnitud_.rename({\"lat\":\"y\", \"lon\":\"x\"})\n",
    "    gdf_tb = sup['geometry']\n",
    "\n",
    "    #Creating new columns polygons with tb 225 \n",
    "    sup['tb_225'] = None\n",
    "\n",
    "    print (\"Estimating Tb spots features: \")    \n",
    "    #Estimating Tb features\n",
    "    for index_t,_dates, progress in zip(gdf_tb.index,sup.time, tqdm.tqdm(range(len(sup.time)))):\n",
    "        _polygon = gdf_tb.geometry.loc[index_t]\n",
    "        coordinates = np.dstack((_polygon.exterior.coords.xy[0], _polygon.exterior.xy[1]))\n",
    "        geometries = [{'type': 'Polygon', 'coordinates': [coordinates[0]]}]    \n",
    "        blob_clipped = data_magnitud_.sel(time=_dates).rio.clip(geometries, gdf_tb.crs, drop=False, invert=False)\n",
    "        \n",
    "        blob_clipped2 = blob_clipped.where(blob_clipped <= tb_overshoot) \n",
    "        #Checking if the DataArray has at least 1 pixels with tb overshoot.\n",
    "        if blob_clipped2.notnull().sum() >= 1:\n",
    "            overshoot = True\n",
    "        else:\n",
    "            overshoot = False\n",
    "\n",
    "        sup.loc[index_t,\"tb_225\"] = overshoot\n",
    "\n",
    "        #Progress bar\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    return sup\n",
    "\n",
    "def clip_tb_pp22(sup, ds, pp_rates, drop_empty_precipitation=True):\n",
    "    \"\"\"\n",
    "    Updated function with error handling and performance improvements.\n",
    "    \"\"\"\n",
    "    # Prepare DataArray\n",
    "    data_magnitud = ds.copy()\n",
    "    #data_magnitud = data_magnitud.chunk({'time': 1, 'lat': 100, 'lon': 100})  # Add lazy loading\n",
    "    data_magnitud = data_magnitud.to_dataframe()\n",
    "    data_magnitud_ = data_magnitud.to_xarray()[\"P\"]\n",
    "    \n",
    "    #Establishing EPSG:4326 - WGS geodetic coordinate system \n",
    "    \n",
    "    #data_magnitud_ = data_magnitud[\"P\"].rio.write_crs(4326)  \n",
    "    data_magnitud_ = data_magnitud_.rio.write_crs(4326) \n",
    "    \n",
    "    #Renaming dimensions \n",
    "    data_magnitud_= data_magnitud_.rename({\"lat\":\"y\", \"lon\":\"x\"}) \n",
    "    gdf_tb = sup['geometry']\n",
    "    \n",
    "    # Initialize columns\n",
    "    sup['pp_10rate'] = None\n",
    "    sup['volum_pp'] = None\n",
    "    sup['mean_pp'] = None\n",
    "    \n",
    "    print(\"Estimating P spots features: \")\n",
    "\n",
    "    for index_t,_dates, progress in zip(gdf_tb.index,sup.time,tqdm.tqdm(range(len(sup.time)))):\n",
    "        _polygon = gdf_tb.geometry.loc[index_t]\n",
    "        coordinates = np.dstack((_polygon.exterior.coords.xy[0], _polygon.exterior.xy[1]))\n",
    "        geometries = [{'type': 'Polygon', 'coordinates': [coordinates[0]]}]\n",
    "\n",
    "        #Applying criteria up to 2 mm/h\n",
    "        blob_clipped = data_magnitud_.sel(time=_dates).rio.clip(geometries, gdf_tb.crs, drop=False, invert=False)\n",
    "        blob_clipped = blob_clipped.where(blob_clipped >= pp_rates)\n",
    "        #Checking if the DataArray has at least 5 pixels with precipitation.\n",
    "        if blob_clipped.notnull().sum() >= 5:\n",
    "            mean_magnitud = round(float(blob_clipped.mean(skipna=True).values),4)\n",
    "        else:\n",
    "            mean_magnitud = np.nan\n",
    "        \n",
    "        #Checking if the DataArray has at least 100km*100km (10 pixels) with precipitation > 2mm/h.\n",
    "        blob_clipped1 = blob_clipped.where(blob_clipped >= 2) \n",
    "        if blob_clipped1.notnull().sum() >= 10:\n",
    "            volume_ppt = True\n",
    "        else:\n",
    "            volume_ppt = False\n",
    "            \n",
    "        #Checking if the DataArray has at least one pixel  with precipitation > 10mm/h\n",
    "        blob_clipped2 = blob_clipped.where(blob_clipped >= 10) \n",
    "        #Checking if the DataArray has at least 1 pixels with precipitation.\n",
    "        if blob_clipped2.notnull().sum() >= 1:\n",
    "            pixels_10mm = True\n",
    "        else:\n",
    "            pixels_10mm = False\n",
    "            \n",
    "        sup.loc[index_t,'volum_pp'] = volume_ppt\n",
    "        sup.loc[index_t,'pp_10rate'] = pixels_10mm\n",
    "        sup.loc[index_t,\"mean_pp\"] = mean_magnitud\n",
    "        #sup['pp_10rate'] = round(sup['pp_10rate'].astype(float),4)\n",
    "        \n",
    "        #Progress bar\n",
    "        time.sleep(0.01)       \n",
    "        \n",
    "        #Dropping polygons that do not meet the precipitation parameters\n",
    "        #in terms of area and amount.\n",
    "    if drop_empty_precipitation == True:\n",
    "        sup = sup[sup['mean_pp'].notna()].reset_index(drop=True)\n",
    "    else: \n",
    "        pass\n",
    "    return sup\n",
    "\n",
    "\n",
    "def min_dist(point, gpd2):\n",
    "    \"\"\"\n",
    "    Function to find the nearest polygon based on a georeferenced point.\n",
    "    \n",
    "    Inputs:\n",
    "    point: Georeferenced point\n",
    "    gpd2: Geodataframe - Polygons in a specific time and region\n",
    "    \n",
    "    Outputs:\n",
    "    Geoseries with the nearest polygon to the georeferenced point.\n",
    "    \"\"\"\n",
    "    gpd = gpd2.copy()\n",
    "    gpd['Dist'] = gpd.apply(lambda row:  point.distance(row.geometry),axis=1)\n",
    "    geoseries = gpd.iloc[gpd['Dist'].argmin()]\n",
    "    return geoseries\n",
    "\n",
    "\n",
    "def finder_msc2(sup, threshold_overlapping_percentage = None):\n",
    "    global msc_counter\n",
    "    \"\"\"\n",
    "    Function for tracking convective systems according to the threshold_overlapping_percentage.\n",
    "    This functions works based on identified convective systems in a period of time.   \n",
    "\n",
    "    Inputs:\n",
    "    sup: GeodataFrame generated in the process \"identify_msc2\"\n",
    "    threshold_overlapping_percentage (float): By default there is no percentage overlap limit \n",
    "    between polygons, however, this can be set. \n",
    "    (ex. 10: This means that the percentage of overlap that is not greater than or equal to 10%, \n",
    "    in reference to the largest area polygon, is not taken into account in the trajectory).\n",
    "    \n",
    "    Outputs:\n",
    "    GeodataFrame which identifies the tracks in the time period of interest\n",
    "    \"\"\"\n",
    "    msc_counter = 1\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    range_sites = sup.index\n",
    "    #Making column belong\n",
    "    sup.loc[sup.index, \"belong\"] = 0\n",
    "    #Making column percentage intersection\n",
    "    sup.loc[sup.index, \"intersection_percentage\"] = np.nan\n",
    "        \n",
    "    #Generating tracks\n",
    "    \n",
    "    print (\"Estimating trajectories: \" )\n",
    "    for isites, progress in zip(range_sites, tqdm.tqdm(range(len(range_sites)))):\n",
    "        # Checking when the record (spot) has not yet been processed or attaching to a track\n",
    "        if sup.at[isites, \"belong\"] == 0:\n",
    "            try:\n",
    "                #Assigning the counter number to identify the track(range 1) \n",
    "                sup.at[isites, \"belong\"] = msc_counter #Index, col  \n",
    "                \n",
    "                #Generating start and end time date (time t and time t+1)\n",
    "                date_initial = sup.loc[isites].time\n",
    "                date_final = date_initial + timedelta(hours=1)\n",
    "                \n",
    "                #Reference spot\n",
    "                poli_ti = sup.loc[isites].to_frame().T\n",
    "                #Establishing EPSG:32718 - UTM zone 18S plane coordinate system \n",
    "                poli_ti = gpd.GeoDataFrame(poli_ti, geometry='geometry', crs=UTM_WORLD_ZONE)\n",
    "                \n",
    "                #Spots one hour later\n",
    "                poli_ti1 = sup.loc[sup.time == date_final]\n",
    "                \n",
    "                #Intersection of reference spot with spots one hour later\n",
    "                intersect = gpd.overlay(poli_ti, poli_ti1, how='intersection') ##cogee el politi y muestra con cuales del siguiente paso se traslapan \n",
    "                                                      \n",
    "                #Nearest intersection point (centroid)\n",
    "                \n",
    "                try: #When there is only one intersection spot\n",
    "                    \n",
    "                    #Estimating the distance between centroids\n",
    "                    dist = min_dist(intersect.centroid, poli_ti1)\n",
    "                                \n",
    "                    #Extracting the spot with the maxima area\n",
    "                    intersect_area_max = max(intersect.area_tb_1.values, intersect.area_tb_2.values)\n",
    "    \n",
    "                    #Estimating percentage of intersection\n",
    "                    intersect_percentage = ((intersect.area/1000000)*100/intersect_area_max)[0].round(1)                \n",
    "                    \n",
    "                    #Condition if threshold overlapping percentage is activated\n",
    "                    #If the spot fails with threshold overlapping percentage is dropped\n",
    "                    if isinstance(threshold_overlapping_percentage, checknumbers.Real):\n",
    "                        if intersect_percentage>= threshold_overlapping_percentage:\n",
    "                            sup.at[isites, \"intersection_percentage\"] = intersect_percentage #Index, col \n",
    "                            #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "\n",
    "                        else:\n",
    "                            sup.drop(isites, inplace = True)\n",
    "                            #print (\"The convective system \" + str(isites) + \" was dropped to the data due intersection is below that the threshold: \"+ str(intersect_percentage)) \n",
    "\n",
    "                    #Condition if threshold overlapping percentage is not activated\n",
    "                    else:\n",
    "                        sup.at[isites, \"intersection_percentage\"] = intersect_percentage #Index, col \n",
    "                        #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "\n",
    "                except: #When there are more than one intersection spot is selected the bigger spot / select the nearest spot\n",
    "                    \n",
    "                    #dimensioning the Geodataframe columns only for the intersected spots\n",
    "                    intersect = intersect[intersect.columns[10:]]  #poligonos con los que se intersecta\n",
    "                    \n",
    "                    #Extracting the intersection spot with the maxima area of intersection\n",
    "                    intersect['area_intersected'] = intersect.area/10**6\n",
    "                    \n",
    "                    a_int = intersect.loc[:, intersect.columns.str.contains('area_intersected')].idxmax().values[0] \n",
    "                    a_pol = intersect.loc[:, intersect.columns.str.contains('area_tb')].idxmax().values[0] \n",
    "                    if a_int == a_pol:\n",
    "                        bigger_polygon = a_int\n",
    "                    else:\n",
    "                        bigger_polygon = a_pol\n",
    "                    \n",
    "                    #Preparing Geodataframe - renaming columns\n",
    "                    interest_polygon = intersect.loc[bigger_polygon].to_frame()\n",
    "                    name_columns = ['intersection_percentage', 'time', 'Tb', 'area_tb', \n",
    "                                    'centroides', 'mean_tb', 'min_tb', 'max_tb', 'mean_pp','max_pp', 'belong', \n",
    "                                    'intersection_percentage', 'geometry','area_intersected']\n",
    "                    interest_polygon_t = interest_polygon.T ##este es el poligono de la interseccion no el siguiente\n",
    "                    interest_polygon_t.columns = name_columns\n",
    "                    covex = gpd.GeoDataFrame(interest_polygon_t, geometry=interest_polygon_t.geometry)\n",
    "\n",
    "                    #Area of the bigger spot                \n",
    "                    area_bigger_polygon = covex.area_tb.values[0]\n",
    "                    \n",
    "        \n",
    "                    #Estimating the distance between centroids  ???               \n",
    "                    dist = min_dist(covex.centroid, poli_ti1)\n",
    "                                        \n",
    "                    #Estimating percentage of intersection between t0 and t+1               \n",
    "                    intersect_percentage = ((covex['area_intersected'].values[0])*100/area_bigger_polygon).round(1)                \n",
    "\n",
    "                    #Condition if threshold overlapping percentage is activated\n",
    "                    #If the spot fails with threshold overlapping percentage is dropped                    \n",
    "                    if isinstance(threshold_overlapping_percentage, checknumbers.Real):\n",
    "                        if intersect_percentage>= threshold_overlapping_percentage:\n",
    "                            sup.at[isites, \"intersection_percentage\"] = intersect_percentage \n",
    "                            #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "                        else:\n",
    "                            sup.drop(isites, inplace = True)\n",
    "                            #print (\"The convective system \" + str(isites) + \" was dropped to the data due intersection is below that the threshold: \"+ str(intersect_percentage)) \n",
    "                    #Condition if threshold overlapping percentage is not activated\n",
    "                    else:\n",
    "                        sup.at[isites, \"intersection_percentage\"] = intersect_percentage \n",
    "                        #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "                    #print (\"Warning: in the tracking of this convective system were more than 1 intersecting polygon: \"  + str(isites))\n",
    "\n",
    "                #Attaching found spot to the track\n",
    "                time_intersect = dist.to_frame().T[\"time\"].values[0]\n",
    "                tb_intersect = dist.to_frame().T[\"Tb\"].values[0]\n",
    "                sup.loc[(sup.time == time_intersect) & (sup.Tb == tb_intersect), \"belong\"] = msc_counter\n",
    "                msc_counter +=1\n",
    "                #print (\"The convective system: \" + str(isites) + \" - belongs to the track: \" +str(int(msc_counter)))\n",
    "            except:\n",
    "               msc_counter +=1\n",
    "\n",
    "        # Checking when the record (spot) has been processed or attaching to a track            \n",
    "        elif sup.at[isites, \"belong\"] in range(1, msc_counter):\n",
    "            try:\n",
    "                #Generating start and end time date (time t and time t+1)                \n",
    "                date_initial = sup.iloc[isites].time\n",
    "                date_final = date_initial + timedelta(hours=1)\n",
    "            \n",
    "                #Reference spot\n",
    "                poli_ti = sup.loc[isites].to_frame().T\n",
    "                #Establishing EPSG:32718 - UTM zone 18S plane coordinate system                 \n",
    "                poli_ti = gpd.GeoDataFrame(poli_ti, geometry='geometry', crs=UTM_WORLD_ZONE)\n",
    "                #Getting the id track previously estimate\n",
    "                index_track = poli_ti.belong\n",
    "                \n",
    "                #Spots one hour later\n",
    "                poli_ti1 = sup.loc[(sup.time == date_final) & (sup.belong == 0)]\n",
    "    \n",
    "                #Intersection of reference spot with spots one hour later\n",
    "                intersect = gpd.overlay(poli_ti, poli_ti1, how='intersection')\n",
    "                \n",
    "                try:#When there is only one intersection spot\n",
    "                    \n",
    "                    #Estimating the distance between centroids\n",
    "                    dist = min_dist(intersect.centroid, poli_ti1)\n",
    "                    \n",
    "                    #Extracting the spot with the maxima area\n",
    "                    intersect_area_max = max(intersect.area_tb_1.values, intersect.area_tb_2.values)\n",
    "    \n",
    "                    #Estimating percentage of intersection                    \n",
    "                    intersect_percentage = ((intersect.area/1000000)*100/intersect_area_max)[0].round(1)                \n",
    "                    #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "                    \n",
    "                    #Condition if threshold overlapping percentage is activated\n",
    "                    #If the spot fails with threshold overlapping percentage is dropped\n",
    "                    if isinstance(threshold_overlapping_percentage, checknumbers.Real):\n",
    "                        if intersect_percentage>= threshold_overlapping_percentage:\n",
    "                            sup.at[isites, \"intersection_percentage\"] = intersect_percentage \n",
    "                            #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "                        else:\n",
    "                            sup.drop(isites, inplace = True)\n",
    "                            #print (\"The convective system \" + str(isites) + \" was dropped to the data due intersection is below that the threshold: \"+ str(intersect_percentage)) \n",
    "                    \n",
    "                    #Condition if threshold overlapping percentage is not activated\n",
    "                    else:\n",
    "                        sup.at[isites, \"intersection_percentage\"] = intersect_percentage \n",
    "                        #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "\n",
    "                except:  #When there are more than one intersection spot is selected the bigger spot\n",
    "                   \n",
    "                    #dimensioning the Geodataframe columns only for the intersected spots\n",
    "                    intersect = intersect[intersect.columns[10:]]\n",
    "                    \n",
    "                    #Extracting the spot with the maxima area\n",
    "                    intersect['area_intersected'] = intersect.area/10**6\n",
    "                    \n",
    "                    a_int = intersect.loc[:, intersect.columns.str.contains('area_intersected')].idxmax().values[0] \n",
    "                    a_pol = intersect.loc[:, intersect.columns.str.contains('area_tb')].idxmax().values[0] \n",
    "                    if a_int == a_pol:\n",
    "                        bigger_polygon = a_int\n",
    "                    else:\n",
    "                        bigger_polygon = a_pol \n",
    "\n",
    "                                        \n",
    "                    #Preparing Geodataframe - renaming columns                    \n",
    "                    interest_polygon = intersect.loc[bigger_polygon].to_frame()\n",
    "                    name_columns = ['intersection_percentage', 'time', 'Tb', 'area_tb', \n",
    "                                    'centroides', 'mean_tb', 'min_tb', 'max_tb', 'mean_pp','max_pp', 'belong', \n",
    "                                    'intersection_percentage', 'geometry', 'area_intersected']\n",
    "                    interest_polygon_t = interest_polygon.T\n",
    "                    interest_polygon_t.columns = name_columns\n",
    "                    \n",
    "                    #Area of the bigger spot                \n",
    "                    covex = gpd.GeoDataFrame(interest_polygon_t, geometry=interest_polygon_t.geometry)\n",
    "\n",
    "                    #Area of the bigger spot                \n",
    "                    area_bigger_polygon = covex.area_tb.values[0]\n",
    "\n",
    "                    #Estimating the distance between centroids                                                                                            \n",
    "                    #intersect = intersect.loc[bigger_polygon].to_frame().T  #interest_polygon.T\n",
    "                    dist = min_dist(covex.centroid, poli_ti1)\n",
    "                \n",
    "                    #Estimating percentage of intersection                    \n",
    "                    intersect_percentage = ((covex['area_intersected'].values[0])*100/area_bigger_polygon).round(1)                 \n",
    "\n",
    "                    #Condition if threshold overlapping percentage is activated\n",
    "                    #If the spot fails with threshold overlapping percentage is dropped                    \n",
    "                    if isinstance(threshold_overlapping_percentage, checknumbers.Real):\n",
    "                        if intersect_percentage>= threshold_overlapping_percentage:\n",
    "                            sup.at[isites, \"intersection_percentage\"] = intersect_percentage \n",
    "                            #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "                        else:\n",
    "                            sup.drop(isites, inplace = True)\n",
    "                            #print (\"The convective system \" + str(isites) + \" was dropped to the data due intersection is below that the threshold: \"+ str(intersect_percentage)) \n",
    "                    #Condition if threshold overlapping percentage is not activated\n",
    "                    else:\n",
    "                        sup.at[isites, \"intersection_percentage\"] = intersect_percentage \n",
    "                        #print (\"There was a \" + str(intersect_percentage) + \"% intersection\") \n",
    "                #print (\"Warning: in the tracking of this convective system were more than 1 intersecting polygon: \"  + str(isites))\n",
    "\n",
    "                #Attaching found spot to the track\n",
    "                time_intersect = dist.to_frame().T[\"time\"].values[0]\n",
    "                tb_intersect = dist.to_frame().T[\"Tb\"].values[0]\n",
    "                sup.loc[(sup.time == time_intersect) & (sup.Tb == tb_intersect), \"belong\"] = index_track.values[0]\n",
    "                #print (\"The convective system: \" + str(isites) + \" - belongs to the track: \" +str(int(index_track.values[0])))\n",
    "            except:\n",
    "                pass   \n",
    "            \n",
    "        #Progress bar\n",
    "        time.sleep(0.01)    \n",
    "       \n",
    "    #Transforming plane coordinates to geodesics  coordinates   \n",
    "    sup[\"centroides\"] = sup[\"centroides\"].to_crs(4326)\n",
    "    sup = sup.to_crs(4326) \n",
    "    \n",
    "    #Creating an original index\n",
    "    sup[\"id_gdf\"] = None\n",
    "    sup[\"id_gdf\"] = sup.index\n",
    "    return sup\n",
    "    \n",
    "def resume_track(sup, initial_time_hour):\n",
    "    \"\"\"\n",
    "    Function for calculating characteristics associated with each tracking\n",
    "    average speed, average distance, average direction, \n",
    "    total duration, average area, total distance traveled, total time traveled\n",
    "    \n",
    "    Inputs:\n",
    "    sup: DataFrame generated in process \"finder_msc2\"\n",
    "    initial_time_hour: Default is 0, but could chnage based on in a specific hour duration tracks\n",
    "    \n",
    "    Outputs:\n",
    "    DataFrame containing tracks and features\n",
    "    \"\"\"      \n",
    "\n",
    "    #Preparing Dataframe   \n",
    "    NUEVODF = sup.copy()\n",
    "    NUEVODF = NUEVODF.set_index(['belong', 'id_gdf']).sort_index()\n",
    "\n",
    "    #Replacing old id for spots and tracks based on alphanumeric code 16 and 20\n",
    "    #characteres respectively    \n",
    "    new_belong = []\n",
    "    new_track_id = []\n",
    "    \n",
    "    for track_id in NUEVODF.index.levels[1]:\n",
    "        new_track_id.append(str(uuid.uuid4())[-22:])\n",
    "    dic_replace_track_id = dict(zip(NUEVODF.index.levels[1].values, new_track_id))\n",
    "    \n",
    "    for belong_id in NUEVODF.index.levels[0]:\n",
    "        new_belong.append(str(uuid.uuid4())[:13])\n",
    "    dic_replace_belong = dict(zip(NUEVODF.index.levels[0].values, new_belong))\n",
    "    \n",
    "    reg_sup_res =  NUEVODF.reset_index()\n",
    "    \n",
    "    reg_sup_res.belong = reg_sup_res.belong.replace(dic_replace_belong)\n",
    "    reg_sup_res.id_gdf = reg_sup_res.id_gdf.replace(dic_replace_track_id)\n",
    "       \n",
    "    reg_sup_res = reg_sup_res.drop(labels=['Tb'],axis=1)\n",
    "       \n",
    "    reg_sup = reg_sup_res.set_index([\"belong\" , \"id_gdf\"]).sort_index()\n",
    "    \n",
    "    #Attaching to the dataframe total duration and total distance for each spot. Each spot \n",
    "    #has the register of the features of his own track\n",
    "    reg_sup[\"total_duration\"] = None\n",
    "    #reg_sup[\"total_distance\"] = None\n",
    "    \n",
    "    count_df = reg_sup_res.groupby(by = [reg_sup_res.belong]).count()\n",
    "    #sum_df = reg_sup_res.groupby(by = [reg_sup_res.belong]).sum()\n",
    "    \n",
    "    for _b in count_df.index: \n",
    "    \n",
    "        count_value = count_df.loc[_b,\"id_gdf\"]\n",
    "        #sum_value = sum_df.loc[_b,\"distancia\"]\n",
    "\n",
    "        reg_sup.loc[_b, \"total_duration\"] = count_value\n",
    "        #reg_sup.loc[_b, \"total_distance\"] = sum_value\n",
    "\n",
    "    #Estimating track mean velocity      \n",
    "    reg_deep_convection = reg_sup[reg_sup['total_duration'] >= initial_time_hour]\n",
    "\n",
    "    #extraigo tambien el original \n",
    "    reg_deep_convection['time'] = pd.to_datetime(reg_deep_convection['time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    groupbelong2 = reg_deep_convection.groupby(level=[0])\n",
    "    sorttime_deep_convect = groupbelong2.apply(lambda x: x.sort_values(by='time'))\n",
    "    sorttime_deep_convect = sorttime_deep_convect[['time', 'geometry', 'area_tb', 'centroides', 'tb_225', 'pp_10rate','volum_pp','total_duration']]\n",
    "    sorttime_deep_convect =  sorttime_deep_convect.droplevel(0)\n",
    "    \n",
    "    #Saving as .csv the resultsfor MCS\n",
    "    # sorttime_reg_sup.to_csv(pathResultados + \"resume_MCS2_\"+str(reg_sup.time.min())[:-7]+\"_\"+str(reg_sup.time.max())[:-7]+\".csv\")\n",
    "    sorttime_deep_convect.to_csv(pathResultados + \"resume_DeepConvection2_\"+str(reg_sup.time.min())[:-7]+\"_\"+str(reg_sup.time.max())[:-7]+\".csv\")\n",
    " \n",
    "    return sorttime_deep_convect\n",
    "\n",
    "def plot_folium(resume, location, path_save):\n",
    "    \"\"\"\n",
    "    function for plotting tracks results in folium map. \n",
    "\n",
    "    Inputs:\n",
    "    * resume: GeoDataFrame, data related with the tracks and MCS's.\n",
    "    * location list (lat, lon), location for center the map_folium.\n",
    "    * path_save: str, path where the .html folium map will be saved   \n",
    "    \n",
    "    Outputs:\n",
    "    * the .html folium map will be open with the librarie \"webbrowser\"\n",
    "    * path_saved: str, path where the .html was saved.\n",
    "    \"\"\"\n",
    "    m = folium.Map(location=location, zoom_start=5, tiles='CartoDB positron')\n",
    "    \n",
    "    df = resume.reset_index()\n",
    "    \n",
    "    for i in df.belong.unique():\n",
    "        #Sorting index by time\n",
    "        tracks = df.loc[df.belong == i].reset_index()\n",
    "        tracks  = tracks.set_index(\"time\").sort_index()\n",
    "        tracks = tracks.reset_index()\n",
    "        for idn, r in tracks.iterrows():\n",
    "\n",
    "            sim_geo = gpd.GeoSeries(r['geometry']).simplify(tolerance=0.001)\n",
    "            geo_j = sim_geo.to_json()\n",
    "            geo_j = folium.GeoJson(data=geo_j,\n",
    "                                  style_function=lambda x: {'fillColor': 'orange'})\n",
    "            folium.Popup(r.index).add_to(geo_j)\n",
    "            try: #Tb and P methodlogy\n",
    "                folium.Marker(location=[r['centroides'].y, r['centroides'].x], popup='id_track: {} <br> id_mcs: {} <br> hour_mcs: {} <br> time: {} <br> area[km2]: {} <br> total_duration[h]: {} <br>'.format(r['belong'], r[\"id_gdf\"], idn, r[\"time\"], round(r['area_tb'],1), r[\"total_duration\"])).add_to(m)\n",
    "                extra_name = \"Tb_P_\"\n",
    "            except: #Tb methodlogy\n",
    "                folium.Marker(location=[r['centroides'].y, r['centroides'].x], popup='id_track: {} <br> id_mcs: {} <br> hour_mcs: {} <br> time: {} <br> area[km2]: {} <br> total_duration[h]: {} <br>'.format(r['belong'], r[\"id_gdf\"], idn, r[\"time\"], round(r['area_tb'],1), r[\"total_duration\"])).add_to(m)            \n",
    "                extra_name = \"Tb_\"\n",
    "            geo_j.add_to(m)\n",
    "        \n",
    "    min_time = str(resume.time.min())[:-6].replace(\"-\",\"_\").replace(\" \",\"_\")\n",
    "    max_time = str(resume.time.max())[:-6].replace(\"-\",\"_\").replace(\" \",\"_\")\n",
    "    path_result = path_save+'map_'+extra_name+min_time+\"_\"+max_time+\".html\"\n",
    "    m.save(path_result)\n",
    "    try:\n",
    "        webbrowser.open(path_result)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return path_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6600a3c2-b963-478d-a01b-b9296488ad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se produjo una excepción: These variables cannot be found in this dataset: ['xtime_bnds']\n",
      "Complete data reading 2020-01-20T00:00 - 2020-02-28T23:00\n"
     ]
    }
   ],
   "source": [
    "#reading dataset\n",
    "ds, dates, lon, lat = read_MCSMIP_data(path_obs,run) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbc8ae4b-9d3e-4691-99d3-d9d72427a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spots identification completed\n"
     ]
    }
   ],
   "source": [
    "#Spots identificaction\n",
    "sup = identify_msc2(ds, variables, tb_value, area_value, pp_rates,buffer_threshold)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e326c52a-d18a-4cf3-ba47-45f8c0bc319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Tb spots features: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 60302/60303 [44:12<00:00, 22.73it/s]  \n"
     ]
    }
   ],
   "source": [
    "#Estimating brightness temperature features ()\n",
    "sup = clip_tb(sup, ds, tb_overshoot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01400d67-dcc3-4429-b11c-dbd6b1d2e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating P spots features: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 60302/60303 [1:08:03<00:00, 14.77it/s] \n"
     ]
    }
   ],
   "source": [
    "#Estimating precipitation features. The criteria is based on the variables methodology used in identify_msc2.\n",
    "#Coninue precipitation\n",
    "sup = clip_tb_pp22(sup, ds, pp_rates, drop_empty_precipitation = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c7d1f2b-b8b3-4bd8-a07c-333ddfaa60fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating trajectories: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 46695/46696 [29:46<00:00, 26.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#Tracks identification\n",
    "sup = finder_msc2(sup, threshold_overlapping_percentage = None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce4362b5-67b0-4906-a6ec-7e7f6035a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resume track\n",
    "deep_convection = resume_track(sup, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e25829cb-4e5f-40c7-bf8b-297e8cbf5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import webbrowser\n",
    "# import folium\n",
    "# path_html_folium = plot_folium(deep_convection, location = LOCATION_FOLIUM, path_save = pathResultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Project Feng",
   "language": "python",
   "name": "vrobledod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
